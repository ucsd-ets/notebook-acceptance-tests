# more nodes

More nodes and a second switch for next year

10G switch to add to mix, fileserver setup redundant links

Hardware is new, only 2 legacy APM are 2 GPU boxes

13-15 nodes removed for new hardware

VM nodes running out of torrey pines, 604-608 and master node in IVM2 (sdac)

centos systems with puppet config with kubernetes infrastructure

## centralized

Centralized monitoring team, but have services separate

Cannot make changes regularly with centralized monitoring team

Many groups have direct edit access to act nagios

New mailing group for status notifications, add other members its-linux-alerts.ucsd.edu

## hardware

Setup to help maintain hardware

Individual disk failing is not an issue, if a zfs pool fills up, need to allocate more space

zfs send => sdacs cumulo storage (weekly storage backups)

100TB in cumulo => 7 grand a year, cheaper than aws equivalent ~ 2 full copies of data

ieng6 create modules files ~ 201 only allowed, 202?

10G ports for hardware

1 more GPU node, research has purchased a JBOD fileserver

resilvering onto spare if yank disk

## Outage



